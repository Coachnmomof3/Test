{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Flatten, BatchNormalization, Conv2D, MaxPooling2D, Lambda, Input, AveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "#import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'COVID19', 'NORMAL', 'Viral_Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "# Check filenames \n",
    "image_names=list(os.listdir(\"Database\"))\n",
    "image_names.sort()\n",
    "print(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total Covid images: 1327\n",
      "total Normal images: 1341\n",
      "total Viral_Pneumonia: 1463\n"
     ]
    }
   ],
   "source": [
    "covid_dir=os.path.join(\"Database/COVID19\")\n",
    "norm_dir=os.path.join(\"Database/NORMAL\")\n",
    "pneu_dir=os.path.join(\"Database/Viral_Pneumonia\")\n",
    "\n",
    "\n",
    "print('total Covid images:', len(os.listdir(covid_dir)))\n",
    "print('total Normal images:', len(os.listdir(norm_dir)))\n",
    "print('total Viral_Pneumonia:', len(os.listdir(pneu_dir)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COVID-19 (979).png</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVID-19 (580).png</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVID-19 (996).png</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COVID-19 (646).png</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COVID-19 (216).png</td>\n",
       "      <td>Covid19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0 condition\n",
       "0  COVID-19 (979).png   Covid19\n",
       "1  COVID-19 (580).png   Covid19\n",
       "2  COVID-19 (996).png   Covid19\n",
       "3  COVID-19 (646).png   Covid19\n",
       "4  COVID-19 (216).png   Covid19"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.DataFrame(os.listdir(covid_dir))\n",
    "df[\"condition\"]= \"Covid19\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Viral_Pneumonia    1345\n",
       "NORMAL             1341\n",
       "Covid19            1143\n",
       "Name: condition, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.DataFrame.from_records({\"file_name\":os.listdir(covid_dir),\"condition\": \"Covid19\"})\n",
    "\n",
    "for f in image_names[1:]:\n",
    "    folder_path=\"Database/\" + f\n",
    "    temp_df= pd.DataFrame.from_records({\"file_name\":os.listdir(folder_path),\"condition\": f })\n",
    "    df=df.append(temp_df)\n",
    "    \n",
    "df[\"condition\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import csv to match correct condition\n",
    "# data_csv= pd.read_csv(\"https://raw.githubusercontent.com/Coachnmomof3/UCB_COVID_Prediction_Model/james_demott/connect_core_data_set_csv.csv\")\n",
    "# data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2682 images belonging to 3 classes.\n",
      "Found 1147 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "img_width, img_height=200,200\n",
    "batch_size=128\n",
    "\n",
    "data_dir = \"Database/\"\n",
    "\n",
    "# Rescale images \n",
    "datagen= ImageDataGenerator(rescale=1/255,validation_split=.3,rotation_range=20,\n",
    "                           shear_range=.2,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.2)\n",
    "\n",
    "# Flow training images in batches of 128 using train_data \n",
    "train_generator= datagen.flow_from_directory(\n",
    "                data_dir,\n",
    "                target_size=(img_width,img_height),\n",
    "                batch_size=batch_size,\n",
    "                subset=\"training\",\n",
    "                class_mode=\"categorical\", \n",
    "                classes= [\"COVID19\", \"NORMAL\",\"Viral_Pneumonia\"],\n",
    "                shuffle=True, seed=30)\n",
    "\n",
    "\n",
    "datagen2=ImageDataGenerator(rescale=1/255,validation_split=.3)\n",
    "\n",
    "test_generator=datagen2.flow_from_directory(\n",
    "                data_dir,\n",
    "                target_size=(img_width,img_height),\n",
    "                batch_size=batch_size,\n",
    "                classes= [\"COVID19\", \"NORMAL\",\"Viral_Pneumonia\"],\n",
    "                class_mode=\"categorical\", subset=\"validation\", shuffle=True, seed=30)\n",
    "\n",
    "# train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#   data_dir,\n",
    "#   validation_split=0.2,\n",
    "#   subset=\"training\",\n",
    "#   seed=123,\n",
    "#   image_size=(img_height, img_width),\n",
    "#   batch_size=batch_size)\n",
    "\n",
    "# # Flow test images in \n",
    "\n",
    "\n",
    "# val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#   data_dir,\n",
    "#   validation_split=0.2,\n",
    "#   subset=\"validation\",\n",
    "#   seed=123,\n",
    "#   image_size=(img_height, img_width),\n",
    "#   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "### Using new dataset and with 3 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN MODEL- conv-batch-maxpool-dropout\n",
    "\n",
    "classifier=Sequential()\n",
    "classifier.add(Conv2D(32, kernel_size=3, activation=\"relu\", input_shape=(200,200,3)))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(strides=(2,2)))\n",
    "classifier.add(Dropout(.3))\n",
    "\n",
    "classifier.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(strides=(2,2)))\n",
    "classifier.add(Dropout(.5))\n",
    "\n",
    "classifier.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(strides=(2,2)))\n",
    "classifier.add(Dropout(.4))\n",
    "\n",
    "classifier.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(strides=(2,2)))\n",
    "classifier.add(Dropout(.3))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(512,activation=\"relu\"))\n",
    "classifier.add(Dense(128,activation=\"relu\"))\n",
    "classifier.add(Dropout(.4))\n",
    "\n",
    "classifier.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 198, 198, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 198, 198, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 99, 99, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 99, 99, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 97, 97, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 97, 97, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 21, 21, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 21, 21, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               3277312   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 3,409,699\n",
      "Trainable params: 3,409,315\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "20/20 [==============================] - 77s 4s/step - loss: 1.5852 - categorical_accuracy: 0.6449 - val_loss: 4.0032 - val_categorical_accuracy: 0.5312\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - 75s 4s/step - loss: 0.4950 - categorical_accuracy: 0.8148 - val_loss: 4.2001 - val_categorical_accuracy: 0.3164\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - 77s 4s/step - loss: 0.4074 - categorical_accuracy: 0.8453 - val_loss: 5.5221 - val_categorical_accuracy: 0.2930\n"
     ]
    }
   ],
   "source": [
    "classifier.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=\"adam\", \n",
    "              metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "total_sample=train_generator.n\n",
    "batch_size=128\n",
    "\n",
    "history= classifier.fit_generator(\n",
    "            train_generator, steps_per_epoch=int(total_sample/batch_size),\n",
    "            epochs=3,\n",
    "            validation_data=test_generator, \n",
    "            validation_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 2\n",
    "classifier= Sequential()\n",
    "\n",
    "# First convolution layer\n",
    "classifier.add(Conv2D(32, (3,3), input_shape=(200,200,3),activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Second convolution layer\n",
    "classifier.add(Conv2D(64,(3,3),activation=\"relu\"))\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Third convolution layer\n",
    "classifier.add(Conv2D(64, (3,3), activation=\"relu\"))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Fourth convolution layer\n",
    "classifier.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Fifth convolution layer\n",
    "classifier.add(Conv2D(128, (3,3), activation=\"relu\"))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Flatten the results to feed into a dense layer\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# 128 neuron in the fully-connected layer\n",
    "classifier.add(Dense(units = 128 , activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "\n",
    "# 14 output neurons for 14 classes with the softmax activation\n",
    "classifier.add(Dense(units = 3 , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "20/20 [==============================] - 98s 5s/step - loss: 0.6247 - accuracy: 0.7844 - val_loss: 1.5648 - val_accuracy: 0.4350\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - 101s 5s/step - loss: 0.3223 - accuracy: 0.8896 - val_loss: 2.9193 - val_accuracy: 0.4682\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - 100s 5s/step - loss: 0.2713 - accuracy: 0.9056 - val_loss: 4.0489 - val_accuracy: 0.4446\n"
     ]
    }
   ],
   "source": [
    "# Model 2, with augmentation\n",
    "classifier.compile(\n",
    "  optimizer='adam',\n",
    "  loss=\"categorical_crossentropy\",\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "total_sample= train_generator.n\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "history= classifier.fit_generator(\n",
    "            train_generator, steps_per_epoch=int(total_sample/batch_size),\n",
    "            epochs=3,\n",
    "            validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3\n",
    "# GENERATE MODEL\n",
    "classifier= Sequential()\n",
    "\n",
    "# First convolution layer\n",
    "classifier.add(Conv2D(32, (3,3), input_shape=(200,200,3),activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Second convolution layer\n",
    "classifier.add(Conv2D(32,(3,3),activation=\"relu\"))\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "# Flatten the results to feed into a dense layer\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# 128 neuron in the fully-connected layer\n",
    "classifier.add(Dense(128 , activation = 'relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "\n",
    "# 14 output neurons for 14 classes with the softmax activation\n",
    "classifier.add(Dense(3 , activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "20/20 [==============================] - 88s 4s/step - loss: 5.6278 - accuracy: 0.6496 - val_loss: 0.9794 - val_accuracy: 0.4159\n",
      "Epoch 2/3\n",
      "20/20 [==============================] - 85s 4s/step - loss: 0.6697 - accuracy: 0.7835 - val_loss: 1.0927 - val_accuracy: 0.5484\n",
      "Epoch 3/3\n",
      "20/20 [==============================] - 78s 4s/step - loss: 0.5371 - accuracy: 0.8391 - val_loss: 5.0745 - val_accuracy: 0.5214\n"
     ]
    }
   ],
   "source": [
    "# Model 3, with augmentation\n",
    "classifier.compile(\n",
    "  optimizer='adam',\n",
    "  loss=\"categorical_crossentropy\",\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "# total_valsample=test_generator.n\n",
    "total_sample= train_generator.n\n",
    "batch_size=128\n",
    "\n",
    "history= classifier.fit_generator(\n",
    "            train_generator, steps_per_epoch=int(total_sample/batch_size),\n",
    "            epochs=3,\n",
    "            validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4- Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2893 images belonging to 3 classes.\n",
      "Found 1238 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "img_width, img_height=200,200\n",
    "batch_size=128\n",
    "\n",
    "data_dir = \"Database/\"\n",
    "\n",
    "# Rescale images \n",
    "datagen= ImageDataGenerator(rescale=1/255,validation_split=.3,rotation_range=20,\n",
    "                           shear_range=.2,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.2,\n",
    "                           preprocessing_function=preprocess_input)\n",
    "\n",
    "# Flow training images in batches of 128 using train_data \n",
    "train_generator= datagen.flow_from_directory(\n",
    "                data_dir,\n",
    "                target_size=(64,64),\n",
    "                batch_size=batch_size,\n",
    "                subset=\"training\",\n",
    "                class_mode=\"categorical\", \n",
    "                classes= [\"COVID19\", \"NORMAL\",\"Viral_Pneumonia\"],\n",
    "                shuffle=True, seed=30)\n",
    "\n",
    "\n",
    "datagen2=ImageDataGenerator(rescale=1/255,validation_split=.3,\n",
    "                            preprocessing_function=preprocess_input)\n",
    "\n",
    "test_generator=datagen2.flow_from_directory(\n",
    "                data_dir,\n",
    "                target_size=(64,64),\n",
    "                batch_size=batch_size,\n",
    "                classes= [\"COVID19\", \"NORMAL\",\"Viral_Pneumonia\"],\n",
    "                class_mode=\"categorical\", subset=\"validation\", shuffle=True, seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4\n",
    "\n",
    "datagen= ImageDataGenerator(rescale=1/255,validation_split=.3)\n",
    "\n",
    "classifier=VGG16(weights=\"imagenet\", include_top=False, input_shape=(64,64,3))\n",
    "\n",
    "for layer in classifier.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "x=Flatten()(classifier.output)\n",
    "x=Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "model=Model(inputs=classifier.input, outputs=x)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.4525 - accuracy: 0.8448 - val_loss: 0.5112 - val_accuracy: 0.7577\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 60s 3s/step - loss: 0.4217 - accuracy: 0.8524 - val_loss: 0.4714 - val_accuracy: 0.7908\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3934 - accuracy: 0.8651 - val_loss: 0.4735 - val_accuracy: 0.7738\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3677 - accuracy: 0.8687 - val_loss: 0.4576 - val_accuracy: 0.7892\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3665 - accuracy: 0.8756 - val_loss: 0.5012 - val_accuracy: 0.7633\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3486 - accuracy: 0.8781 - val_loss: 0.4749 - val_accuracy: 0.7811\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3413 - accuracy: 0.8846 - val_loss: 0.4487 - val_accuracy: 0.8021\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3351 - accuracy: 0.8821 - val_loss: 0.4375 - val_accuracy: 0.8150\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3198 - accuracy: 0.8915 - val_loss: 0.4313 - val_accuracy: 0.8142\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3181 - accuracy: 0.8911 - val_loss: 0.4993 - val_accuracy: 0.7674\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 60s 3s/step - loss: 0.3190 - accuracy: 0.8875 - val_loss: 0.4141 - val_accuracy: 0.8174\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3117 - accuracy: 0.8908 - val_loss: 0.4305 - val_accuracy: 0.8110\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2973 - accuracy: 0.8976 - val_loss: 0.4308 - val_accuracy: 0.8174\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3012 - accuracy: 0.8955 - val_loss: 0.4551 - val_accuracy: 0.8029\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.3032 - accuracy: 0.8937 - val_loss: 0.4797 - val_accuracy: 0.7916\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2832 - accuracy: 0.9005 - val_loss: 0.4466 - val_accuracy: 0.8045\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 60s 3s/step - loss: 0.2870 - accuracy: 0.8940 - val_loss: 0.4499 - val_accuracy: 0.8069\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2773 - accuracy: 0.9005 - val_loss: 0.3984 - val_accuracy: 0.8360\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2735 - accuracy: 0.9052 - val_loss: 0.4526 - val_accuracy: 0.8045\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2672 - accuracy: 0.9103 - val_loss: 0.4917 - val_accuracy: 0.7884\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2752 - accuracy: 0.9052 - val_loss: 0.4758 - val_accuracy: 0.7900\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2790 - accuracy: 0.8998 - val_loss: 0.3560 - val_accuracy: 0.8619\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.2585 - accuracy: 0.9114 - val_loss: 0.3751 - val_accuracy: 0.8481\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 66s 3s/step - loss: 0.2687 - accuracy: 0.9092 - val_loss: 0.3729 - val_accuracy: 0.8522\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2701 - accuracy: 0.9089 - val_loss: 0.4283 - val_accuracy: 0.8231\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2535 - accuracy: 0.9114 - val_loss: 0.3741 - val_accuracy: 0.8481\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2426 - accuracy: 0.9222 - val_loss: 0.3703 - val_accuracy: 0.8506\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2570 - accuracy: 0.9085 - val_loss: 0.3798 - val_accuracy: 0.8481\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 64s 3s/step - loss: 0.2472 - accuracy: 0.9172 - val_loss: 0.3491 - val_accuracy: 0.8643\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 63s 3s/step - loss: 0.2582 - accuracy: 0.9132 - val_loss: 0.3646 - val_accuracy: 0.8578\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 62s 3s/step - loss: 0.2447 - accuracy: 0.9123 - val_loss: 0.3865 - val_accuracy: 0.8489\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 60s 3s/step - loss: 0.2496 - accuracy: 0.9146 - val_loss: 0.4081 - val_accuracy: 0.8344\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 61s 3s/step - loss: 0.2369 - accuracy: 0.9208 - val_loss: 0.3876 - val_accuracy: 0.8489\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2557 - accuracy: 0.9136 - val_loss: 0.4022 - val_accuracy: 0.8320\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2433 - accuracy: 0.9215 - val_loss: 0.3716 - val_accuracy: 0.8546\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2484 - accuracy: 0.9146 - val_loss: 0.3654 - val_accuracy: 0.8562\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 60s 3s/step - loss: 0.2501 - accuracy: 0.9107 - val_loss: 0.3793 - val_accuracy: 0.8465\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2354 - accuracy: 0.9190 - val_loss: 0.4022 - val_accuracy: 0.8328\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 60s 3s/step - loss: 0.2449 - accuracy: 0.9151 - val_loss: 0.3548 - val_accuracy: 0.8603\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2343 - accuracy: 0.9197 - val_loss: 0.4156 - val_accuracy: 0.8231\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2381 - accuracy: 0.9110 - val_loss: 0.3987 - val_accuracy: 0.8360\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2231 - accuracy: 0.9204 - val_loss: 0.3611 - val_accuracy: 0.8578\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2389 - accuracy: 0.9125 - val_loss: 0.3881 - val_accuracy: 0.8417\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 60s 3s/step - loss: 0.2399 - accuracy: 0.9168 - val_loss: 0.4131 - val_accuracy: 0.8263\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 60s 3s/step - loss: 0.2306 - accuracy: 0.9241 - val_loss: 0.3469 - val_accuracy: 0.8691\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2376 - accuracy: 0.9143 - val_loss: 0.3669 - val_accuracy: 0.8554\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2450 - accuracy: 0.9165 - val_loss: 0.3564 - val_accuracy: 0.8627\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2324 - accuracy: 0.9193 - val_loss: 0.3768 - val_accuracy: 0.8506\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2306 - accuracy: 0.9172 - val_loss: 0.3866 - val_accuracy: 0.8425\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 59s 3s/step - loss: 0.2364 - accuracy: 0.9179 - val_loss: 0.3962 - val_accuracy: 0.8425\n"
     ]
    }
   ],
   "source": [
    "total_sample= train_generator.n\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "history= model.fit_generator(\n",
    "            train_generator, steps_per_epoch=int(total_sample/batch_size),\n",
    "            epochs=50,\n",
    "            validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
